# 并发理论基础

> :pencil2: Behind every successful man there's a lot of unsuccessful years.

![](../img/编程语言/并发编程理论基础.png)

## 并发编程的挑战

并发编程的挑战主要有：

- 线程的上下文切换
- 资源的限制

### 上下文切换

即使是单核处理器也支持多线程执行代码，CPU 通过给每个线程分配 CPU 时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以 CPU 通过不停切换线程运行，让我们感觉多个线程是同时进行的，时间片一般是几十毫秒。

单核处理器多个线程执行，其实是属于并发，不是真正意义上并行，实际上每个任务在单核处理上，还是属于串行运行的。

> [!tip]
> 这里引出了两个概念：
>
> - 并行：把每一个任务分配给每一个处理器独立完成。在同一时间点，任务一定是同时运行。
> - 并发：把任务在不同的时间点交给处理器进行处理。在同一时间点，任务并不会同时运行。

CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。

### 减少上下文切换

- 无锁编程
- CAS算法
- 使用最少线程
- 协程

### 资源的限制

资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。

### 资源限制引发的问题

在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。

### 解决资源限制引发的问题

- 解决硬件资源，使用集群分担压力
- 解决软件资源，使用池化技术
- 根据不同的资源情况调整并发度

## CPU多级缓存

由于CPU的运算速度比主存（物理内存）的存取速度快很多，为了提高处理速度，现代CPU不直接和主存进行通信，而是在CPU和主存之间设计了多层的Cache（高速缓存），越靠近CPU的高速缓存越快，容量也越小。

![](../img/编程语言/CPU缓存结构图.png)

根据以上 CPU 缓存结构图，可以看出 CPU 高速缓存有 L1、L2 和 L3（也就是一级缓存、二级缓存、三级缓存）。越靠近 CPU 的高速缓存读取越快，容量也越小。所以 L1 高速缓存容量很小，但存储速度最快，并且仍然只能被一个单独的 CPU 内核使用。 L3 缓存能被同一个 CPU 芯片板上的所有CPU内核共享。最后，系统还拥有一块主存，由系统所有CPU共享。

> [!tip]
>
> 拥有 L3 高速缓存的CPU，CPU 存储数据的命中率可达 95%，也就是说只有不到 5% 的数据需从主存中读取。

----

**CPU通过高速缓存进行数据读取有以下优势：**

- 写缓冲区可以保证指令流水线持续运行，可以避免由于CPU停顿下来等待向内存写入数据而产生的延迟。
- 通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总线的占用。

## 并发编程三大问题

> 由于 CPU 的多级缓存，导致了并发编程的可见性和有序性问题。

### 原子性

所谓原子操作，就是“不可中断的一个或一系列操作”，是指不会被线程调度机制打断的操作。这种操作一旦开始，就一直运行到结束，中间不会有任何线程的切换。

### 可见性

一个线程对共享变量的修改，另一个线程能够立刻可见，我们称该共享变量具备内存可见性。

### 有序性

所谓程序的有序性，是指程序按照代码的先后顺序执行。如果程序执行的顺序与代码的先后顺序不同，并导致了错误的结果，即发生了有序性问题。

## CPU缓存一致性

> 由于每个 CPU 都有自己的缓存，同一份数据可能被缓存在多个 CPU 内核中，那么它们的值可能会不一样，就可能发生了内存的可见性的问题。

### 总线锁和缓存锁

为了解决内存的可见性问题，CPU主要提供了两种解决办法：总线锁和缓存锁。

**1. 总线锁**

操作系统提供了总线锁机制。前端总线（也叫CPU总线）是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号，通过地址总线发送地址信号指定其要访问的部件，通过数据总线实现双向传输。

![](../img/编程语言/CPU总线锁.png)

在多CPU的系统中，当其中一个CPU要对共享主存进行操作时，在总线上发出一个LOCK#信号，这个信号使得其他CPU无法通过总线来访问共享主存中的数据，总线锁把CPU和主存之间的通信锁住了，这使得锁定期间，其他CPU不能操作其他主存地址的数据，总线锁的开销比较大，这种机制显然是不合适的。

> [!warning]
>
> 某一个CPU访问主存时，总线锁把CPU和主存的通信给锁住了，其他CPU不能操作其他主存地址的数据，使得效率低下，开销较大。

----

**2. 缓存锁**

相比总线锁，缓存锁降低了锁的粒度。为了达到数据访问的一致，需要各个 CPU 在访问高速缓存时遵循一些协议，在存取数据时根据协议来操作，常见的协议有 MSI、MESI、MOSI 等。最常见的就是 MESI 协议。

![](../img/编程语言/CPU缓存锁.png)

就整体而言，缓存一致性机制就是当某CPU对高速缓存中的数据进行操作之后，通知其他CPU放弃存储在它们内部的缓存数据，或者从主存中重新读取。

为了提高处理速度，CPU 不直接和主存进行通信，而是先将系统主存的数据读到内部高速缓存（L1、L2或其他）后再进行操作。但存在操作完不知道何时会写入内存这个问题，即使写回系统中，如果其他 CPU 高速缓存中的值还是旧的，再执行计算就会产生问题。

因此，在多CPU的系统中，为了保证各个CPU的高速缓存中数据的一致性，会实现缓存一致性协议，每个CPU通过嗅探在总线上传播的数据来检查自己的高速缓存中的值是否过期，当CPU发现自己缓存行对应的主存地址被修改时，就会将当前CPU的缓存行设置成无效状态，当CPU对这个数据执行修改操作时，会重新从系统主存中把数据读到CPU的高速缓存中。

----

为了保证高速缓存和内存数据的一致性，有以下两种写入方法：

- **直写：**在数据更新时，同时写入低一级的高速缓存和内存。此模式优点是操作简单，因为所有的数据都会更新到主存，所以其他 CPU 读取主存时都是最新值。此模式的缺点是数据写入速度较慢，因为数据修改之后需要同时写入低一级的高速缓存和主存。

- **回写：**数据更新不会立即反映到主存，而是只写入到高速缓存。只在数据被替换出高速缓存或者变成共享（S）状态时，如果发现数据有变动，才会将最新的数据更新到主存。此模式的的优点是写入速度快，因为发生数据变动时不需要写入主存，所有这种模式占用总线少。此模式的缺点是，实现一致性协议比较复杂。

## 重排序

> 内存屏障又称内存栅栏（Memory Fences），是一系列的CPU指令，它的作用主要是保证特定操作的执行顺序，保障并发执行的有序性，用于解决指令重排序。

为了提高性能，编译器和CPU常常会对指令进行重排序。重排序主要分为两类：编译器重排序和 CPU 重排序。

![](../img/编程语言/重排序.png)

### 编译器重排序

编译器重排序指的是在代码编译阶段进行指令重排，不改变程序执行结果的情况下，为了提升效率，编译器对指令进行乱序（Out-of-Order）的编译。

> [!tip]
>
> 与其等待阻塞指令（如等待缓存刷入）完成，不如先去执行其他指令。与CPU乱序执行相比，编译器重排序能够完成更大范围、效果更好的乱序优化。

### CPU重排序

为了提供 CPU 的执行效率，流水线都是并行处理的，只要两个指令之间不存在**数据依赖**，就可以对这两个指令乱序。

CPU 重排序包括两类：

- 指令重排序：在不影响程序执行结果的情况下，CPU内核采用ILP（Instruction-LevelParallelism，指令级并行运算）技术来将多条指令重叠执行，主要是为了提升效率。如果指令之间不存在数据依赖性，CPU就可以改变语句的对应机器指令的执行顺序，叫作指令级重排序。
- 内存重排序：由于 CPU 高级缓存的存在，数据的读取和更新都需要经过高速缓存，最后才到主存，但是也因此导致了数据不一致的问题。

> 内存重排序是一种伪排序，也就是说只是看起来像在乱序执行而已。